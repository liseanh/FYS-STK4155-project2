\documentclass[a4paper, 11pt, twocolumn]{article}

\usepackage[a4paper, total={6.24in, 8.5in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{float}
\usepackage{array}
\usepackage{xfrac}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{sectsty,textcase}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{enumitem}
\usepackage{hyperref}
\bibliographystyle{plain}
% \renewcommand{\thesubsection}{\alph{subsection})}
\renewcommand{\thesection}{\arabic{section}}
%\renewcommand{\baselinestretch}{1.02}

%\setlength\parindent{0pt}
\setlength{\columnsep}{15pt}
\title{FYS-STK3155/4155 Applied Data Analysis and Machine Learning - Project 2: Classification and Regression }

\author{Lotsberg, Bernhard Nornes \\ Nguyen, Anh-Nguyet Lise \and \url{https://github.com/liseanh/FYS-STK4155-project2/}}
\date{October - November 2019}
\begin{document}
\twocolumn[
  \begin{@twocolumnfalse}
    \maketitle
    \begin{abstract}
      blip blop opnion wrong
    \end{abstract}
  \end{@twocolumnfalse}
]

\section{Introduction}
Classification in statistical analysis is a useful tool, e.g. for predicting outcomes of various situations or classifying and sorting large amounts of data. 
\begin{figure}[H]
	\includegraphics[width=\columnwidth]{figures/regtest.pdf}
	\caption{TEST FIGURE AS PLACEHOLDER, PLEASE REMOVE FROM FINAL VERSION}
\end{figure}

The aim of this project is  to study classification and regression problems through our own implementation of logistic regression and a multilayer perceptron (MLP) in Python. This particular data set has been used in a prior research paper by Yeh, I. C. et al about data mining techniques \cite{origarticle}, and can be downloaded from the \href{https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients}{UCI Machine Learning Repository} \cite{UCI}.
We will use these methods to classify data of credit card clients' default payment from a Taiwanese bank. Additionally, we will use the MLP to solve a regression problem on the Franke function and compare the result with prior regression analysis results of the function using standard least squares.  


\section{Theory}
\subsection{Stochastic Gradient Descent (SGD)}
\the \columnwidth

\subsection{Logistic Regression (LR)}

\subsection{Artificial Neural Networks (ANN)}
In an artificial neural network, something something nodes. The output of the nodes in each layer is given by the value of a chosen activation function $f(z)$. 	
\subsubsection{Multilayer perceptron}
The multilayer perceptron is a feedforward neural network. 

The activation of the $j$th neuron of layer $l$ is defined as 
\begin{equation}
z_j^l = \sum_{i=1}^{M_{l-1}} w_{ij}^la_j^{l-1} + b_j^l,
\end{equation}
where $b_j^l$ and $w_{ij}^l$ are the biases and weights at layer $l$, and $a_j^l=f(z_j^l) $.

To calculate the optimal biases and weights for the problem, we initialize the gradients of the cost function $\mathcal{C}$ with respect to the  weights $W$ and biases $b$ at the output  layer $l=L$ 	and the output error $\delta_L$ as
\begin{flalign}
&\frac{\partial  \mathcal{C}}{\partial w^L_{jk}} = \delta^L_j a_k^{L-1}, \\
&\frac{\partial  \mathcal{C}}{\partial b^L_j} = \delta^L_j,\\
&\delta_j^L= f'(z_j^L)\frac{\partial \mathcal{C}}{\partial a_j^L},
\end{flalign}
before propagating backwards through the hidden layers using the general equations
\begin{flalign}
&\frac{\partial  \mathcal{C}}{\partial w^l_{jk}} = \delta^l_j a_k^{l-1}, \\
&\frac{\partial  \mathcal{C}}{\partial b^l_j} = \delta^l_j,\\
&\delta_j^l= \sum_k \delta_k^{l+1}w_{kj}^{l+1}f'(z_j^l).
\end{flalign}
Looking at these equations, it is clear that the chosen cost function $\mathcal{C}$ should be differentiable. 
\section{Data}
In this paper we are using credit card payment data from a Taiwanese bank downloaded from the \href{https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients}{UCI Machine Learning Repository}. The response variable is a binary variable of default payment with Yes = 1, No = 0.  The original data set consists of 30 000 observations, with X amount of observations with default payments. There are 23 explanatory variables, cited from the original paper they are described as \cite{origarticle}:
\begin{itemize}[leftmargin=5mm, itemsep=0pt,  parsep=1pt]
 \item X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.
\item X2: Gender (1 = male; 2 = female).
\item X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).
\item X4: Marital status (1 = married; 2 = single; 3 = others).
\item X5: Age (year).
\item X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.
\item X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005.
\item X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005.
\end{itemize}

\section{Model evaluation}
\subsection{Regression}
To evaluate the performance of our regression model, we consider the $R^2$ score, given by 
\begin{equation}
    R^2(\bm{y}, \bm{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
    \label{eq:R2}
\end{equation}
where $\bm{y}$ is the given data, $\bm{\hat{y}}$ is the model and ${\bar{y}}$ is the mean value of $\bm{y}$.
\subsection{Classification}
To evaluate the performance of our classification model, we consider the accuracy score, given by 
\begin{equation}
\label{eq:accuracy}
\text{accuracy}=\frac{\sum_{i=1}^nI(t_i=y_i)}{n},
\end{equation}
where $t_i$ is the target, $y_i$ is the model output, $n$ is the number of samples and $I$ is the indicator function, 
\[
I = \begin{cases} 
1, & t_i = y_i\\
0, & t_i \neq y_i
\end{cases} .
\]

\section{Method}



\section{Results}



\section{Discussion}
fill
\section{Conclusion}
fill
\bibliographystyle{iEEEtran}
\bibliography{references}



\end{document}
